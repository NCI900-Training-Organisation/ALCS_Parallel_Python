{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "288c7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "322c9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8dbe3",
   "metadata": {},
   "source": [
    "##### In a normal CUDA workflow we have to allocate the memory on the GPU and the move the data to the GPU memory. In CuPy this is not required, the memory allocation and data movement can be done in a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42ba260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 270 µs, sys: 209 µs, total: 479 µs\n",
      "Wall time: 387 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_gpu_0 = cp.asarray(x_cpu)  # move the ndarray from host mem to GPU0 memeory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56653e7",
   "metadata": {},
   "source": [
    "#### In the past any communication between two GPUs had to go throgh the PCIe card. But now NVIDIA offeres a technology called NVLink. NVLink is a direct GPU-to-GPU interconnect that scales multi-GPU input/output (IO) within a node. This makes GPU-to-GPU transfer (D2D tranfer) much faster than GPU-to-Host (D2H transfer) or Host-to-GPU transfer (H2D transfer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b83abf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 336 µs, sys: 0 ns, total: 336 µs\n",
      "Wall time: 266 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with cp.cuda.Device(1):\n",
    "    x_gpu_1 = cp.asarray(x_gpu_0)  # move the ndarray to GPU0 to GPU1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a7c91",
   "metadata": {},
   "source": [
    "##### There are two ways to fetch the data from the GPU to host ``cupy.ndarray.get()`` or ``cupy.asnumpy``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6f00c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with cp.cuda.Device(0):\n",
    "    x_cpu = cp.asnumpy(x_gpu_0)  # move the array from GPU 0 back to the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "663402b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ee4f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with cp.cuda.Device(1):\n",
    "    x_cpu = x_gpu_1.get()  # move the array from GPU 1 back to the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d4d24c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf4ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
